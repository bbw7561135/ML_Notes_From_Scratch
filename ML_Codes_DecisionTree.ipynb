{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#决策树\n",
    "# 你是否玩过二十个问题的游戏，游戏的规则很简单：参与游戏的一方在脑海里想某个事物，其他参与者向他提问题，只允许提20个问题，\n",
    "# 问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围。或者是酒桌上的猜数游戏，游戏规则有点类似，\n",
    "# 参与游戏的一方在脑海里想一个固定的数值，需要在固定的范围内，其他参与者进行猜测，他会先给出猜测的正确与否，如果正确直接喝酒，\n",
    "# 如果错误，就在下一个参与者开始之前给出新的猜数范围，逐步缩小待猜测事物的范围。\n",
    "\n",
    "# 决策树的工作原理与上面两个游戏类似，用户输入一系列数据，然后给出游戏的答案。\n",
    "# 我们经常使用决策树处理分类问题，近来的调查表明决策树也是最经常使用的数据挖掘算法。\n",
    "# 它之所以如此流行，一个很重要的原因就是不需要了解机器学习的知识，就能搞明白决策树是如何工作的。\n",
    "\n",
    "# 那么如何从一堆原始数据中构造决策树呢？别着急，过程如下：\n",
    "\n",
    "# 首先讨论构造决策树的方法，以及如何编写构造树的Python代码；\n",
    "# 接着提出一些度量算法成功率的方法；\n",
    "# 最后使用递归建立分类器，并且使用Matplotlib绘制决策树图。\n",
    "\n",
    "# 决策树\n",
    "# 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。\n",
    "# 缺点：可能会产生过度匹配问题。\n",
    "# 适用数据类型：数值型和标称型。\n",
    "\n",
    "# 决策树的一般流程\n",
    "# (1) 收集数据：可以使用任何方法。\n",
    "# (2) 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。\n",
    "# (3) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。\n",
    "# (4) 训练算法：构造树的数据结构。\n",
    "# (5) 测试算法：使用经验树计算错误率。\n",
    "# (6) 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。\n",
    "\n",
    "# 决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。\n",
    "# 这一过程对应着对特征空间的划分，也对应着决策树的构建。开始，构建根结点，将所有训练数据都放在根结点。\n",
    "# 选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。\n",
    "# 如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去，\n",
    "# 如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。\n",
    "# 如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。\n",
    "# 最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了一棵决策树。\n",
    "\n",
    "\n",
    "# 从上述过程中就可以看出，决策树的生成是一个递归过程。在决策树基本算法中，有三种情形会导致递归返回\n",
    "# 当前结点包含的样本全属于同一类别，无需划分\n",
    "# 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分\n",
    "# 当前结点包含的样本集合为空，不能划分\n",
    "# 在第二种情形下，我们把当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别。\n",
    "# 在第三种情形下，同样把当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别。\n",
    "# 这两种情形的处理实质不同：第二种情况是在利用当前结点的后验分布，而第三种情况则是把父结点的样本分布作为当前结点的先验分布\n",
    "\n",
    "\n",
    "# 以上方法生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生过拟合现象。\n",
    "# 我们需要对已生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。\n",
    "# 具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。\n",
    "# 如果特征数量很多，也可以在决策树学习开始的时候，对特征进行选择，只留下对训练数据有足够分类能力的特征\n",
    "\n",
    "# 可以看出，决策树学习算法包含特征选择、决策树的生成与决策树的剪枝过程。\n",
    "# 由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型。\n",
    "# 决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。\n",
    "# 决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。\n",
    "\n",
    "# 决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。\n",
    "# 它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。\n",
    "# 分类树具有良好的可读性与分类速度快的优点。分类树在学习时，利用训练数据，根据损失函数最小化的原则建立分类树模型，\n",
    "# 在预测时，对新的数据，利用分类树模型进行分类。决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪\n",
    "\n",
    "# 可以将决策树看成一个if-then规则的集合：由决策树的根结点到叶结点的每一条路径构建一条规则，\n",
    "# 路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。\n",
    "# 决策树的路径或其对应的if-then规则集合具有一个重要的性质——互斥并且完备。\n",
    "# 这就是说，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。\n",
    "# 这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件\n",
    "\n",
    "# 决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分上。\n",
    "# 将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。\n",
    "# 决策树的一条路径对应于划分中的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。\n",
    "# 假设XX为表示特征的随机变量，YY为表示类的随机变量，那么这个条件概率分布可以表示为P（Y∣X）P（Y∣X）。\n",
    "# XX取值于给定划分下单元的集合，YY取值于类的集合。各叶结点（单元）上的条件概率往往偏向某一个类，即属于某一类的概率较大。\n",
    "# 决策树分类时将该结点的实例强行分到条件概率大的那一类去\n",
    "#决策树的预测准确性一般比回归和分类方法弱，但可以通过用集成学习方法组合大量决策树，显著提升树的预测效果\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树要如何构建呢？\n",
    "# 通常，这一过程可以概括为3个步骤：特征选择、决策树的生成和决策树的修剪\n",
    "\n",
    "# 特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率；\n",
    "# 如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。\n",
    "# 经验上扔掉这样的特征对决策树学习的精度影响不大。\n",
    "\n",
    "# 通常特征选择的标准是信息增益(information gain)或信息增益比，为了简单，本文章使用信息增益作为选择特征的标准。\n",
    "# 那么，什么是信息增益？\n",
    "# 在继续讲解之前，先看一组实例，贷款申请样本数据表\n",
    "\n",
    "# ID\t年龄\t有工作\t有自己的房子\t信贷情况\t类别(是否个给贷款)\n",
    "# 1\t青年\t否\t否\t一般\t否\n",
    "# 2\t青年\t否\t否\t好\t否\n",
    "# 3\t青年\t是\t否\t好\t是\n",
    "\n",
    "# 希望通过所给的训练数据学习一个贷款申请的决策树，用以对未来的贷款申请进行分类，\n",
    "# 即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。\n",
    "\n",
    "# 特征选择就是决定用哪个特征来划分特征空间。\n",
    "# 比如，我们通过上述数据表得到两个可能的决策树，分别由两个不同特征的根结点构成 年龄 工作\n",
    "\n",
    "# 现在我们想要决定选择第一个特征还是第二个特征，但是问题是：究竟选择哪个特征更好些？\n",
    "# 这就要求确定选择特征的准则。直观上，如果一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，\n",
    "# 使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。\n",
    "# 信息增益就能够很好地表示这一直观的准则。那么什么是信息增益呢？\n",
    "\n",
    "# 在划分数据集之前之后信息发生的变化成为信息增益，知道如何计算信息增益，\n",
    "# 我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。\n",
    "# 在可以评测哪种数据划分方式是最好的数据划分之前，我们必须学习如何计算 信息增益。\n",
    "# 集合信息的度量方式称为 香农熵 或者简称为 熵，这个名字来源于信息论之父克劳德·香农\n",
    "#在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量 熵大表示不确定性大 反之则小\n",
    "# 一般地，熵H(Y)H(Y)与条件熵H(Y∣X)H(Y∣X)之差称为互信息（mutual information）。\n",
    "# 决策树学习中的信息增益等价于训练数据集中类与特征的互信息。\n",
    "# 决策树学习应用信息增益准则选择特征。给定训练数据集DD和特征a∗a \n",
    "# 经验熵H(D)H(D)表示对数据集DD进行分类的不确定性。而经验条件熵H(D∣a∗)H(D∣a )表示在特征a∗a \n",
    "# 给定的条件下对数据集DD进行分类的不确定性。那么它们的差，即信息增益，就表示由于特征a∗a \n",
    "# 而使得对数据集DD的分类的不确定性减少的程度。显然，对于数据集DD而言，信息增益依赖于特征，\n",
    "# 不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力\n",
    "# 根据信息增益准则的特征选择方法：对训练数据集（或子集）DD，\n",
    "# 计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。\n",
    "\n",
    "# 下面就可以准备编写代码来进行验证，在这之前，先对数据集进行属性标注并计算经验熵\n",
    "\n",
    "# 年龄：0代表青年，1代表中年，2代表老年；\n",
    "# 有工作：0代表否，1代表是；\n",
    "# 有自己的房子：0代表否，1代表是；\n",
    "# 信贷情况：0代表一般，1代表好，2代表非常好；\n",
    "# 类别(是否给贷款)：no代表否，yes代表是\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 'no'], [0, 0, 0, 1, 'no'], [0, 1, 0, 1, 'yes'], [0, 1, 1, 0, 'yes'], [0, 0, 0, 0, 'no'], [1, 0, 0, 0, 'no'], [1, 0, 0, 1, 'no'], [1, 1, 1, 1, 'yes'], [1, 0, 1, 2, 'yes'], [1, 0, 1, 2, 'yes'], [2, 0, 1, 2, 'yes'], [2, 0, 1, 1, 'yes'], [2, 1, 0, 1, 'yes'], [2, 1, 0, 2, 'yes'], [2, 0, 0, 0, 'no']]\n",
      "0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    无\n",
    "Returns:\n",
    "    dataSet - 数据集\n",
    "    labels - 分类属性\n",
    "\"\"\"\n",
    "# 函数说明:创建测试数据集\n",
    "def createDataSet():\n",
    "    dataSet = [[0, 0, 0, 0, 'no'],#数据集 第一列年龄 第二列是否有工作 第三列是否有房子 第四列是否已经有贷款\n",
    "            [0, 0, 0, 1, 'no'],\n",
    "            [0, 1, 0, 1, 'yes'],\n",
    "            [0, 1, 1, 0, 'yes'],\n",
    "            [0, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 1, 'no'],\n",
    "            [1, 1, 1, 1, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 1, 'yes'],\n",
    "            [2, 1, 0, 1, 'yes'],\n",
    "            [2, 1, 0, 2, 'yes'],\n",
    "            [2, 0, 0, 0, 'no']]\n",
    "    labels = ['年龄', '有工作', '有自己的房子', '信贷情况']#分类属性\n",
    "    return dataSet, labels#返回数据集和分类属性\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    shannonEnt - 经验熵(香农熵)\n",
    "\"\"\"\n",
    "# 函数说明:计算给定数据集的经验熵(香农熵)\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntires = len(dataSet)                       #返回数据集的行数\n",
    "    labelCounts = {}                                #保存每个标签(Label)出现次数的字典\n",
    "    for featVec in dataSet:                         #对每组特征向量进行统计\n",
    "        currentLabel = featVec[-1]                  #提取标签(Label)信息\n",
    "        if currentLabel not in labelCounts.keys():  #如果标签(Label)没有放入统计次数的字典,添加进去\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1              #Label计数\n",
    "    shannonEnt = 0.0                                #经验熵(香农熵)\n",
    "    for key in labelCounts:                         #计算香农熵\n",
    "        prob = float(labelCounts[key]) / numEntires #选择该标签(Label)的概率\n",
    "        shannonEnt -= prob * log(prob, 2)           #利用公式计算\n",
    "    return shannonEnt                               #返回经验熵(香农熵)\n",
    "\n",
    "\n",
    "\n",
    "dataSet, features = createDataSet()\n",
    "print(dataSet)\n",
    "print(calcShannonEnt(dataSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个特征的增益为0.083\n",
      "第1个特征的增益为0.324\n",
      "第2个特征的增益为0.420\n",
      "第3个特征的增益为0.363\n",
      "最优特征索引值:2\n"
     ]
    }
   ],
   "source": [
    "# 如何选择特征，需要看 信息增益。也就是说，信息增益 是相对于特征而言的，信息增益 越大，特征对最终的分类结果影响也就越大，\n",
    "# 所以我们应该选择对最终分类结果影响最大的那个特征作为分类特征。在讲解信息增益定义之前，还需要明确一个概念，\n",
    "# 条件熵。熵我们知道是什么，条件熵 又是个什么鬼？条件熵 H(Y|X) 表示在已知随机变量X的条件下随机变量Y的不确定性，\n",
    "# 随机变量X给定的条件下随机变量Y的 条件熵(conditional entropy) H(Y|X)\n",
    "\n",
    "# 接下来说说 信息增益，前面也提到了，信息增益 是相对于特征而言的。\n",
    "# 所以，特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差\n",
    "\n",
    "# 一般地，熵H(D)与条件熵H(D|A)之差称为 互信息(mutual information)。\n",
    "# 决策树学习中的 信息增益 等价于训练数据集中类与特征的 互信息\n",
    "\n",
    "#H(D)可以理解为D的不确定度 而H(D\\A)可以理解为给定A条件后 D的不确定度\n",
    "#两者相减即为信息增益g(D\\A) 因为H(D)固定 那么g(D\\A)完全取决于A 也就是机器学习中的不同特征\n",
    "#这里也就是有无房子 工作 年龄这些因素 如果g(D\\A)很小 说明H(D\\A)和H(D)很接近 说明A对于D的不确定性影响不大\n",
    "#也就是说A并不影响D的取值 或者说A不是有效特征 如果g(D\\A)很大 说明H(D\\A)很小 说明A对于D影响很大 A给定后 D不确定性很小\n",
    "#说明A对于D影响很大 所以用来做特征是合适的\n",
    "#总结来说就是信息增益g(D\\A)越大 说明特征A对D的分类影响越大 A特征应该选为节点的特征 不论是根节点和分结点都遵循这个办法\n",
    "#一直递归这个过程 直到特征遍历完 或者余下的变量的分类都是同一类 也就是所谓的叶节点\n",
    "#或者我们如何决定什么时候停止分 我们可以这样设置\n",
    "#当某个节点的信息熵小于某个阈值时我们就停止对这个节点的分支操作，那么此节点也就成为了叶子节点\n",
    "#也就是信息熵比较小的时候 可以认为比较确定的情况下 就不需要继续进行分支操作\n",
    "#最终我们需要确定每个叶子结点的类别，即叶子结点中的样本集中，占比最大的那一个类别便是当前叶子节点的类别\n",
    "#当新来一个样本我们只需要按照决策树从顶层向下逐步判断，看样本最终落入那个叶子结点，所落入的叶子结点的类别便是当前样本的预测类别\n",
    "\n",
    "#计算不同的特征的信息增益\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    dataSet - 待划分的数据集\n",
    "    axis - 划分数据集的特征\n",
    "    value - 需要返回的特征的值\n",
    "Returns:\n",
    "    无\n",
    "\"\"\"\n",
    "# 函数说明:按照给定特征划分数据集\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    retDataSet = []                                #创建返回的数据集列表\n",
    "    for featVec in dataSet:                        #遍历数据集\n",
    "        if featVec[axis] == value:                #axis是特征所在的列 value是set里面的子特征 这里统计某个子特征的出现频率\n",
    "            reducedFeatVec = featVec[:axis]        #去掉axis特征\n",
    "            reducedFeatVec.extend(featVec[axis+1:])#将符合条件的添加到返回的数据集\n",
    "            retDataSet.append(reducedFeatVec)      #比如数据是 1 0 1 1 第一列要划分的数据是1 那么 把 0 1 1和其他满足第一列是\n",
    "                                                   #1的存到一起 这一子数据集就是根据第一列是1划分 第一列是0的 划分到另外个数据集\n",
    "    return retDataSet                              #返回划分后的数据集\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    bestFeature - 信息增益最大的(最优)特征的索引值\n",
    "\"\"\"\n",
    "# 函数说明:选择最优特征\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1                     #特征数量 最后一列是标签 不是特征 要减去\n",
    "    baseEntropy = calcShannonEnt(dataSet)                 #计算整体数据集的香农熵\n",
    "    bestInfoGain = 0.0                                    #信息增益\n",
    "    bestFeature = -1                                      #最优特征的索引值\n",
    "    for i in range(numFeatures):                          #遍历所有特征\n",
    "        #获取dataSet的第i个所有特征\n",
    "        featList = [example[i] for example in dataSet]    #把某一列的所有数据集中到featList\n",
    "        uniqueVals = set(featList)                         #创建set集合{},元素不可重复 \n",
    "        #比如年龄 5年轻 5中年 5老年 这里set后就剩下 年轻 中年 老年 三个子特征\n",
    "        newEntropy = 0.0                                   #经验条件熵\n",
    "        for value in uniqueVals:                           #计算信息增益\n",
    "            subDataSet = splitDataSet(dataSet, i, value)   #subDataSet划分后的子集\n",
    "            prob = len(subDataSet) / float(len(dataSet))   #计算子集的概率\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)#根据公式计算经验条件熵 其实就是主特征概率子特征的经验熵 \n",
    "        infoGain = baseEntropy - newEntropy                #信息增益\n",
    "        print(\"第%d个特征的增益为%.3f\" % (i, infoGain))     #打印每个特征的信息增益\n",
    "        if (infoGain > bestInfoGain):                      #计算信息增益\n",
    "            bestInfoGain = infoGain                        #更新信息增益，找到最大的信息增益\n",
    "            bestFeature = i                                #记录信息增益最大的特征的索引值\n",
    "    return bestFeature                                     #返回信息增益最大的特征的索引值\n",
    "\n",
    "\n",
    "\n",
    "dataSet, features = createDataSet()\n",
    "print(\"最优特征索引值:\" + str(chooseBestFeatureToSplit(dataSet)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个特征的增益为0.083\n",
      "第1个特征的增益为0.324\n",
      "第2个特征的增益为0.420\n",
      "第3个特征的增益为0.363\n",
      "第0个特征的增益为0.252\n",
      "第1个特征的增益为0.918\n",
      "第2个特征的增益为0.474\n",
      "{'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}\n"
     ]
    }
   ],
   "source": [
    "#最后，比较特征的信息增益大小，发现特征A3(有自己的房子)的信息增益值最大，所以选择A3作为最优特征\n",
    "#ID3算法的核心是在决策树各个结点上对应 信息增益 准则选择特征，递归地构建决策树，ID3相当于用极大似然法进行概率模型的选择\n",
    "#具体方法是：\n",
    "# 从根结点(root node)开始，对结点计算所有可能的特征的信息增益，\n",
    "#选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点；\n",
    "# 再对子结点递归地调用以上方法，构建决策树；\n",
    "# 直到所有特征的信息增益均很小或没有特征可以选择为止，最后得到一个决策树。\n",
    "\n",
    "# 上面得到了特征A3(有自己的房子)的信息增益值最大，所以选择特征A3作为根结点的特征\n",
    "# 它将训练集D划分为两个子集D1(A3取值为”是”)和D2(A3取值为”否”)\n",
    "# 由于D1只有同一类的样本点，所以它成为一个叶结点，结点的类标记为“是”(看数据可知 有房子给贷款的概率是100%)\n",
    "# 对D2则需要从特征A1(年龄)，A2(有工作)和A4(信贷情况)中选择新的特征，计算各个特征的信息增益\n",
    "\n",
    "#根据计算，选择信息增益最大的特征A2(有工作)作为结点的特征。\n",
    "#由于A2有两个可能取值，从这一结点引出两个子结点：一个对应”是”(有工作)的子结点，包含3个样本，\n",
    "#它们属于同一类（这里没有房子有工作的三个样本都给贷款了 所以是同一类），所以这是一个叶结点，类标记为”是”；\n",
    "#另一个是对应”否”(无工作)的子结点，包含6个样本，它们也属于同一类（没有给贷款），所以这也是一个叶结点，类标记为”否”。\n",
    "#这样就生成了一个决策树，该决策树只用了两个特征(有两个内部结点)，生成的决策树如下图所示：\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    classList - 类标签列表\n",
    "Returns:\n",
    "    sortedClassCount[0][0] - 出现此处最多的元素(类标签)\n",
    "\"\"\"\n",
    "# 函数说明:统计classList中出现此处最多的元素(类标签)\n",
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:#统计classList中每个元素出现的次数\n",
    "        if vote not in classCount.keys():classCount[vote] = 0   \n",
    "        classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.items(), key = operator.itemgetter(1), reverse = True)#根据字典的值降序排序\n",
    "    return sortedClassCount[0][0]#返回classList中出现次数最多的元素\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    dataSet - 训练数据集\n",
    "    labels - 分类属性标签\n",
    "    featLabels - 存储选择的最优特征标签\n",
    "Returns:\n",
    "    myTree - 决策树\n",
    "\"\"\"\n",
    "# 函数说明:创建决策树\n",
    "def createTree(dataSet, labels, featLabels):\n",
    "    classList = [example[-1] for example in dataSet]       #取分类标签(是否放贷:yes or no)\n",
    "    if classList.count(classList[0]) == len(classList):    #如果类别完全相同则停止继续划分\n",
    "        return classList[0]\n",
    "    if len(dataSet[0]) == 1:   #只有一列数据肯定是标签     #意思就是已遍历完所有特征\n",
    "        return majorityCnt(classList)                     #意思遍历完所有特征后 有些结点下面还出现是否都存在的情况\n",
    "                                #也就是没有叶节点 此时就统计标签里面的次数 取频率高的那个作为结果\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)           #选择最优特征 bestFeat是最优特征的索引\n",
    "    bestFeatLabel = labels[bestFeat]                       #最优特征的标签\n",
    "    featLabels.append(bestFeatLabel)\n",
    "    myTree = {bestFeatLabel:{}}                            #根据最优特征的标签生成树\n",
    "    del(labels[bestFeat])                                  #删除已经使用特征标签\n",
    "    featValues = [example[bestFeat] for example in dataSet]#得到训练集中所有最优特征的属性值\n",
    "    uniqueVals = set(featValues)                           #去掉重复的属性值\n",
    "    for value in uniqueVals:                               #遍历特征，创建决策树。                       \n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), labels, featLabels)#递归调用\n",
    "    return myTree\n",
    "\n",
    "\n",
    "\n",
    "dataSet, labels = createDataSet()\n",
    "featLabels = []\n",
    "myTree = createTree(dataSet, labels, featLabels)\n",
    "print(myTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#可见，决策树构建完成了。很明显这个决策树看着别扭，虽然能看懂，但是如果多点的结点，就不一定了。\n",
    "#下面我们使用强大的Matplotlib绘制决策树\n",
    "\n",
    "# 可视化需要用到的函数：\n",
    "\n",
    "# getNumLeafs：获取决策树叶子结点的数目\n",
    "# getTreeDepth：获取决策树的层数\n",
    "# plotNode：绘制结点\n",
    "# plotMidText：标注有向边属性值\n",
    "# plotTree：绘制决策树\n",
    "# createPlot：创建绘制面板\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    myTree - 决策树\n",
    "Returns:\n",
    "    maxDepth - 决策树的层数\n",
    "\"\"\"\n",
    "# 函数说明:获取决策树的层数\n",
    "def getTreeDepth(myTree):\n",
    "    maxDepth = 0                                          #初始化决策树深度\n",
    "    #python3中myTree.keys()返回的是dict_keys,不在是list,\n",
    "    #所以不能使用myTree.keys()[0]的方法获取结点属性，可以使用list(myTree.keys())[0]\n",
    "    firstStr = next(iter(myTree))                         #第一层是根节点 不算进层数里面 第二层起算 \n",
    "    secondDict = myTree[firstStr]                         #获取下一个字典\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__=='dict':        #测试该结点是否为字典，如果不是字典，代表此结点为叶子结点\n",
    "            thisDepth = 1 + getTreeDepth(secondDict[key])\n",
    "        else:   thisDepth = 1\n",
    "        if thisDepth > maxDepth: maxDepth = thisDepth     #更新层数\n",
    "    return maxDepth\n",
    "\n",
    "print(getTreeDepth(myTree))\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    myTree - 决策树\n",
    "Returns:\n",
    "    numLeafs - 决策树的叶子结点的数目\n",
    "\"\"\"\n",
    "# 函数说明:获取决策树叶子结点的数目\n",
    "def getNumLeafs(myTree):\n",
    "    numLeafs = 0                                   #初始化叶子\n",
    "    #python3中myTree.keys()返回的是dict_keys,不在是list,\n",
    "    #所以不能使用myTree.keys()[0]的方法获取结点属性，可以使用list(myTree.keys())[0]\n",
    "    firstStr = next(iter(myTree))                  #第一层是根节点            \n",
    "    secondDict = myTree[firstStr]                 #获取下一组字典\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__=='dict':#测试该结点是否为字典，如果不是字典，代表此结点为叶子结点\n",
    "            numLeafs += getNumLeafs(secondDict[key])\n",
    "        else:   numLeafs +=1\n",
    "    return numLeafs\n",
    "\n",
    "print(getNumLeafs(myTree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个特征的增益为0.083\n",
      "第1个特征的增益为0.324\n",
      "第2个特征的增益为0.420\n",
      "第3个特征的增益为0.363\n",
      "第0个特征的增益为0.252\n",
      "第1个特征的增益为0.918\n",
      "第2个特征的增益为0.474\n",
      "{'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "    nodeTxt - 结点名\n",
    "    centerPt - 文本位置\n",
    "    parentPt - 标注的箭头位置\n",
    "    nodeType - 结点格式\n",
    "Returns:\n",
    "    无\n",
    "\"\"\"\n",
    "# 函数说明:绘制结点\n",
    "def plotNode(nodeTxt, centerPt, parentPt, nodeType):\n",
    "    arrow_args = dict(arrowstyle=\"<-\")                                      #定义箭头格式\n",
    "    createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords='axes fraction',#绘制结点\n",
    "        xytext=centerPt, textcoords='axes fraction',\n",
    "        va=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args)\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    cntrPt、parentPt - 用于计算标注位置\n",
    "    txtString - 标注的内容\n",
    "Returns:\n",
    "    无\n",
    "\"\"\"\n",
    "# 函数说明:标注有向边属性值\n",
    "def plotMidText(cntrPt, parentPt, txtString):\n",
    "    xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]#计算标注位置                   \n",
    "    yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]\n",
    "    createPlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\", rotation=30)\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    myTree - 决策树(字典)\n",
    "    parentPt - 标注的内容\n",
    "    nodeTxt - 结点名\n",
    "Returns:\n",
    "    无\n",
    "\"\"\"\n",
    "# 函数说明:绘制决策树\n",
    "def plotTree(myTree, parentPt, nodeTxt):\n",
    "    decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\")                                   #设置结点格式\n",
    "    leafNode = dict(boxstyle=\"round4\", fc=\"0.8\")                                         #设置叶结点格式\n",
    "    numLeafs = getNumLeafs(myTree)                                                       #获取决策树叶结点数目，\n",
    "                                                                                        #决定了树的宽度\n",
    "    depth = getTreeDepth(myTree)                                                         #获取决策树层数\n",
    "    firstStr = next(iter(myTree))                                                        #下个字典                                                 \n",
    "    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)#中心位置\n",
    "    plotMidText(cntrPt, parentPt, nodeTxt)                                               #标注有向边属性值\n",
    "    plotNode(firstStr, cntrPt, parentPt, decisionNode)                                   #绘制结点\n",
    "    secondDict = myTree[firstStr]                                                        #下一个字典，\n",
    "                                                                                         #也就是继续绘制子结点\n",
    "    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD                                  #y偏移\n",
    "    for key in secondDict.keys():                               \n",
    "        if type(secondDict[key]).__name__=='dict':                                       #测试该结点是否为字典，\n",
    "                                                                                         #如果不是字典，\n",
    "                                                                                        #代表此结点为叶子结点\n",
    "            plotTree(secondDict[key],cntrPt,str(key))                                    #不是叶结点，\n",
    "                                                                                        #递归调用继续绘制\n",
    "        else:                                                                            #如果是叶结点，绘制叶结点，\n",
    "                                                                                         #并标注有向边属性值                                             \n",
    "            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW\n",
    "            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)\n",
    "            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))\n",
    "    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    inTree - 决策树(字典)\n",
    "Returns:\n",
    "    无\n",
    "\"\"\"\n",
    "# 函数说明:创建绘制面板\n",
    "def createPlot(inTree):\n",
    "    fig = plt.figure(1, facecolor='white')                     #创建fig\n",
    "    fig.clf()                                                  #清空fig\n",
    "    axprops = dict(xticks=[], yticks=[])\n",
    "    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)#去掉x、y轴\n",
    "    plotTree.totalW = float(getNumLeafs(inTree))               #获取决策树叶结点数目\n",
    "    plotTree.totalD = float(getTreeDepth(inTree))              #获取决策树层数\n",
    "    plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0; #x偏移\n",
    "    plotTree(inTree, (0.5,1.0), '')                            #绘制决策树\n",
    "    plt.show()                                                 #显示绘制结果     \n",
    "\n",
    "\n",
    "\n",
    "dataSet, labels = createDataSet()\n",
    "featLabels = []\n",
    "myTree = createTree(dataSet, labels, featLabels)\n",
    "print(myTree)  \n",
    "createPlot(myTree) #画图就是给自己看个明白 无需细究 知道如何使用就好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个特征的增益为0.083\n",
      "第1个特征的增益为0.324\n",
      "第2个特征的增益为0.420\n",
      "第3个特征的增益为0.363\n",
      "第0个特征的增益为0.252\n",
      "第1个特征的增益为0.918\n",
      "第2个特征的增益为0.474\n",
      "有自己的房子\n",
      "{0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}\n",
      "有工作\n",
      "{0: 'no', 1: 'yes'}\n",
      "放贷\n"
     ]
    }
   ],
   "source": [
    "#依靠训练数据构造了决策树之后，可以将它用于实际数据的分类。在执行数据分类时，需要决策树以及用于构造树的标签向量；\n",
    "#然后，程序比较测试数据与决策树上的数值，递归执行该过程直到进入叶子结点；最后将测试数据定义为叶子结点所属的类型\n",
    "\n",
    "# 在构建决策树的代码，可以看到，有个featLabels参数。它是用来记录各个分类结点的，在用决策树做预测的时候，\n",
    "# 按顺序输入需要的分类结点的属性值即可。举个例子，比如用上述已经训练好的决策树做分类，\n",
    "# 只需要提供这个人是否有房子，是否有工作这两个信息即可，无需提供其他冗余的信息。\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    inputTree - 已经生成的决策树\n",
    "    featLabels - 存储选择的最优特征标签\n",
    "    testVec - 测试数据列表，顺序对应最优特征标签\n",
    "Returns:\n",
    "    classLabel - 分类结果\n",
    "\"\"\"\n",
    "# 函数说明:使用决策树分类\n",
    "def classify(inputTree, featLabels, testVec):\n",
    "    firstStr = next(iter(inputTree))    #获取决策树结点 对应的分类标签\n",
    "    print(firstStr)\n",
    "    secondDict = inputTree[firstStr]      #下一个字典\n",
    "    print(secondDict)\n",
    "    featIndex = featLabels.index(firstStr)   #找出标签所在的索引                                           \n",
    "    for key in secondDict.keys():\n",
    "        if testVec[featIndex] == key: #同样索引时候 测试数据如果吻合字典的键\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "                classLabel = classify(secondDict[key], featLabels, testVec) #主要是myTree这个奇葩字典需要仔细理解\n",
    "            else: classLabel = secondDict[key] #下一层不是字典 之间把值赋给classLabel 也就是yes或者no\n",
    "    return classLabel\n",
    "\n",
    "\n",
    "\n",
    "dataSet, labels = createDataSet()\n",
    "featLabels = []\n",
    "myTree = createTree(dataSet, labels, featLabels)\n",
    "testVec = [0,1]#测试数据\n",
    "result = classify(myTree, featLabels, testVec)\n",
    "if result == 'yes':\n",
    "    print('放贷')\n",
    "if result == 'no':\n",
    "    print('不放贷')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造决策树是很耗时的任务，即使处理很小的数据集，如前面的样本数据，也要花费几秒的时间，\n",
    "# 如果数据集很大，将会耗费很多计算时间。然而用创建好的决策树解决分类问题，则可以很快完成。\n",
    "# 因此，为了节省计算时间，最好能够在每次执行分类时调用已经构造好的决策树\n",
    "\n",
    "# 为了解决这个问题，需要使用Python模块pickle序列化对象。\n",
    "# 序列化对象可以在磁盘上保存对象，并在需要的时候读取出来。\n",
    "# 假设我们已经得到决策树\n",
    "# {‘有自己的房子’: {0: {‘有工作’: {0: ‘no’, 1: ‘yes’}}, 1: ‘yes’}}，使用pickle.dump存储决策树\n",
    "\n",
    "import pickle\n",
    "\n",
    "# pickle提供了一个简单的持久化功能。可以将对象以文件的形式存放在磁盘上。\n",
    "# pickle模块只能在python中使用，python中几乎所有的数据类型（列表，字典，集合，类等）都可以用pickle来序列化\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    inputTree - 已经生成的决策树\n",
    "    filename - 决策树的存储文件名\n",
    "Returns:\n",
    "    无\n",
    "\"\"\"\n",
    "# 函数说明:存储决策树\n",
    "def storeTree(inputTree, filename):\n",
    "    with open(filename, 'wb') as fw:\n",
    "        pickle.dump(inputTree, fw)\n",
    "\n",
    "\n",
    "\n",
    "myTree = {'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}\n",
    "storeTree(myTree, 'classifierStorage.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}\n"
     ]
    }
   ],
   "source": [
    "#运行代码，在该Python文件的相同目录下，会生成一个名为classifierStorage.txt的txt文件，\n",
    "#这个是个二进制存储的文件，会生成即可，下次使用的时候\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    filename - 决策树的存储文件名\n",
    "Returns:\n",
    "    pickle.load(fr) - 决策树字典\n",
    "\"\"\"\n",
    "# 函数说明:读取决策树\n",
    "def grabTree(filename):\n",
    "    fr = open(filename, 'rb')\n",
    "    return pickle.load(fr)\n",
    "\n",
    "\n",
    "\n",
    "myTree = grabTree('classifierStorage.txt')\n",
    "print(myTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 至此决策树的基本原理已经介绍完毕，要说明的是，以上介绍的只是决策树当中的ID3算法，在ID3算法提出之后，\n",
    "# 相继又提出了C4.5算法和CART，与ID3算法不同的是，C4.5算法采用的特征选择指标是信息增益率而不是信息增益，\n",
    "# 其他原理同ID3相同。实际上信息增益率的计算方法也相当简单，就是信息增益与特征的熵的比值；\n",
    "# 而CART（Class and Regression Tree）从名字看出来即可做分类又可做回归，与ID3不同的是，作为分类树时，\n",
    "# 其选择划分节点的特征的依据是基尼系数，选择使得划分后数据集基尼系数最小的那个特征作为节点的划分的特征。\n",
    "# 如果是构建回归树，则划分节点的特征的选择是依据划分后子节点的方差，我们尽可能选择使得方差最小的划分特征，\n",
    "# 我们往往使用启发式的方式来搜索划分特征以及划分特征的值，即依次遍历各个维度的特征，当遍历到当前维度特征时，\n",
    "# 依次遍历当前特征的所有值作为划分值从而选出使得子节点方差最小的划分特征以及特征的值。\n",
    "\n",
    "\n",
    "# 决策树学习本质上是从训练数据集中归纳出一组分类规则。\n",
    "# 与训练数据集不相矛盾的决策树（即能对训练数据进行正确分类的决策树）可能有多个，也可能一个也没有。\n",
    "# 我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。\n",
    "# 从另一个角度看，决策树学习是由训练数据集估计条件概率模型。\n",
    "# 基于特征空间划分的类的条件概率模型有无穷多个，我们选择的条件概率模型\n",
    "# 应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
